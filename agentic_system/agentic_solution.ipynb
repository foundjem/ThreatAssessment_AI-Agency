{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A multi-agent RAG system, intelligent and autonomous, capable of:\n",
    "\n",
    "- Dynamically refining queries (via LLM).\n",
    "- Retrieving and downloading documents.\n",
    "- Extracting and ranking relevant information.\n",
    "- Intelligently visualizing complex relationships.\n",
    "- Proactively detecting, analyzing, and responding to security threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Autonomous Multi-Agent RAG System with GNN Visualization\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from scholarly import scholarly\n",
    "import pdfplumber\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Advanced LLM Query Refinement Agent\n",
    "def refine_query(initial_query):\n",
    "    prompt = f\"Refine this query for academic research retrieval: '{initial_query}'\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Step 2: Semantic Scholar Retrieval Agent\n",
    "def query_semantic_scholar(query, limit=5):\n",
    "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": query, \"limit\": limit, \"fields\": \"openAccessPdf\"}\n",
    "    response = requests.get(api_url, params=params).json()\n",
    "    return [p['openAccessPdf']['url'] for p in response.get(\"data\", []) if p.get('openAccessPdf')]\n",
    "\n",
    "# Step 3: Google Scholar Retrieval Agent\n",
    "def query_google_scholar(refined_query, num_results=5):\n",
    "    search_results = scholarly.search_pubs(refined_query)\n",
    "    pdf_urls = []\n",
    "    for _ in range(num_results):\n",
    "        pub = next(search_results, None)\n",
    "        if pub and 'eprint_url' in pub:\n",
    "            pdf_urls.append(pub['eprint_url'])\n",
    "    return pdf_urls\n",
    "\n",
    "# Step 4: PDF Download Agent\n",
    "def download_pdfs(pdf_urls, folder):\n",
    "    for idx, pdf_url in enumerate(pdf_urls):\n",
    "        try:\n",
    "            pdf_response = requests.get(pdf_url)\n",
    "            pdf_response.raise_for_status()\n",
    "            filename = os.path.join(folder, f'document_{idx}.pdf')\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(pdf_response.content)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "# Step 5: Extract texts and embed PDFs\n",
    "def extract_texts_and_embed(folder):\n",
    "    texts = []\n",
    "    for pdf_file in os.listdir(folder):\n",
    "        with pdfplumber.open(os.path.join(folder, pdf_file)) as pdf:\n",
    "            text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "            texts.append(text)\n",
    "    embeddings = SentenceTransformer('all-MiniLM-L6-v2').encode(texts)\n",
    "    return texts, embeddings\n",
    "\n",
    "# Step 6: Rank and Explain Top Documents\n",
    "def rank_and_explain(query, texts, embeddings):\n",
    "    query_emb = model.encode([query])\n",
    "    scores = util.cos_sim(query_emb, embeddings)[0]\n",
    "    ranked_indices = scores.argsort(descending=True)[:3]\n",
    "\n",
    "    explainer = LimeTextExplainer()\n",
    "    explanations = [explainer.explain_instance(texts[i], lambda x: model.encode(x), num_features=5).as_list() for i in ranked_indices]\n",
    "\n",
    "    return [(texts[i], explanations[idx]) for idx, i in enumerate(ranked_indices)]\n",
    "\n",
    "# Step 7: Intelligent GNN Graph Visualization\n",
    "def intelligent_gnn_visualization(texts, embeddings):\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    edge_index = torch.tensor([[i, j] for i in range(len(texts)) for j in range(len(texts)) if sim_matrix[i, j] > 0.5 and i != j], dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    gcn = GCNConv(data.num_features, 2)\n",
    "    embedding_2d = gcn(data.x, data.edge_index).detach().numpy()\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for i, coords in enumerate(embedding_2d):\n",
    "        G.add_node(f'Doc_{i}', pos=(coords[0], coords[1]))\n",
    "\n",
    "    for i, j in edge_index.t().numpy():\n",
    "        G.add_edge(f'Doc_{i}', f'Doc_{j}', weight=sim_matrix[i, j])\n",
    "\n",
    "    pos = {node: (data['pos'][0], data['pos'][1]) for node, data in G.nodes(data=True)}\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', edge_color='gray')\n",
    "    plt.title(\"GNN-powered Intelligent Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Step 8: Orchestrator Agent (Detection, Reasoning & Response)\n",
    "def orchestrator(query, texts):\n",
    "    if any(term in query.lower() for term in [\"attack\", \"threat\", \"vulnerability\", \"exploit\"]):\n",
    "        analysis = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=f\"Analyze the security threat in the context: {' '.join(texts)}\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        ).choices[0].text.strip()\n",
    "        return f\"Threat detected! Recommended actions: {analysis}\"\n",
    "    return \"No threats detected.\"\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    pdf_urls = query_semantic_scholar(refined_query) + query_google_scholar(refined_query)\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "    texts, embeddings = extract_texts_and_embed(DOCUMENTS_FOLDER)\n",
    "    ranked_docs_with_explanations = rank_and_explain(refined_query, texts, embeddings)\n",
    "    for doc, explanation in ranked_docs_with_explanations:\n",
    "        print(f\"Document snippet: {doc[:300]}\\nExplanation: {explanation}\\n\")\n",
    "    intelligent_gnn_visualization(texts, embeddings)\n",
    "    decision = orchestrator(refined_query, texts)\n",
    "    print(f\"Orchestrator Decision: {decision}\")\n",
    "\n",
    "# Example Execution\n",
    "initial_query = \"Tactics, Techniques, and Procedures in ML Security\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Autonomous Multi-Agent RAG System with GNN Visualization and Re-ranking\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from scholarly import scholarly\n",
    "import pdfplumber\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Advanced LLM Query RefineËœment Agent\n",
    "def refine_query(initial_query):\n",
    "    prompt = f\"Refine this query for academic research retrieval: '{initial_query}'\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Step 2: Semantic Scholar Retrieval Agent\n",
    "def query_semantic_scholar(query, limit=5):\n",
    "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": query, \"limit\": limit, \"fields\": \"openAccessPdf\"}\n",
    "    response = requests.get(api_url, params=params).json()\n",
    "    return [p['openAccessPdf']['url'] for p in response.get(\"data\", []) if p.get('openAccessPdf')]\n",
    "\n",
    "# Step 3: Google Scholar Retrieval Agent\n",
    "def query_google_scholar(refined_query, num_results=5):\n",
    "    search_results = scholarly.search_pubs(refined_query)\n",
    "    pdf_urls = []\n",
    "    for _ in range(num_results):\n",
    "        pub = next(search_results, None)\n",
    "        if pub and 'eprint_url' in pub:\n",
    "            pdf_urls.append(pub['eprint_url'])\n",
    "    return pdf_urls\n",
    "\n",
    "# Step 4: PDF Download Agent\n",
    "def download_pdfs(pdf_urls, folder):\n",
    "    for idx, pdf_url in enumerate(pdf_urls):\n",
    "        try:\n",
    "            pdf_response = requests.get(pdf_url)\n",
    "            pdf_response.raise_for_status()\n",
    "            filename = os.path.join(folder, f'document_{idx}.pdf')\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(pdf_response.content)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "# Step 5: Extract texts and embed PDFs\n",
    "def extract_texts_and_embed(folder):\n",
    "    texts = []\n",
    "    for pdf_file in os.listdir(folder):\n",
    "        with pdfplumber.open(os.path.join(folder, pdf_file)) as pdf:\n",
    "            text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "            texts.append(text)\n",
    "    embeddings = SentenceTransformer('all-MiniLM-L6-v2').encode(texts)\n",
    "    return texts, embeddings\n",
    "\n",
    "# Step 6: Reranking Agent\n",
    "def rerank_documents(query, texts, embeddings, top_k=5):\n",
    "    query_embedding = model.encode([query])\n",
    "    similarity_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "    top_indices = similarity_scores.argsort(descending=True)[:top_k]\n",
    "    return [(texts[i], similarity_scores[i].item()) for i in top_indices]\n",
    "\n",
    "# Step 7: Explain Top Documents\n",
    "def explain_top_documents(top_documents):\n",
    "    explainer = LimeTextExplainer()\n",
    "    explanations = [explainer.explain_instance(doc, lambda x: model.encode(x), num_features=5).as_list() for doc, _ in top_documents]\n",
    "    return explanations\n",
    "\n",
    "# Step 8: Intelligent GNN Graph Visualization\n",
    "def intelligent_gnn_visualization(texts, embeddings):\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    edge_index = torch.tensor([[i, j] for i in range(len(texts)) for j in range(len(texts)) if sim_matrix[i, j] > 0.5 and i != j], dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    gcn = GCNConv(data.num_features, 2)\n",
    "    embedding_2d = gcn(data.x, data.edge_index).detach().numpy()\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for i, coords in enumerate(embedding_2d):\n",
    "        G.add_node(f'Doc_{i}', pos=(coords[0], coords[1]))\n",
    "\n",
    "    for i, j in edge_index.t().numpy():\n",
    "        G.add_edge(f'Doc_{i}', f'Doc_{j}', weight=sim_matrix[i, j])\n",
    "\n",
    "    pos = {node: (data['pos'][0], data['pos'][1]) for node, data in G.nodes(data=True)}\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', edge_color='gray')\n",
    "    plt.title(\"GNN-powered Intelligent Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Step 9: Orchestrator Agent (Detection, Reasoning & Response)\n",
    "def orchestrator(query, texts):\n",
    "    if any(term in query.lower() for term in [\"attack\", \"threat\", \"vulnerability\", \"exploit\"]):\n",
    "        analysis = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=f\"Analyze the security threat in the context: {' '.join(texts)}\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        ).choices[0].text.strip()\n",
    "        return f\"Threat detected! Recommended actions: {analysis}\"\n",
    "    return \"No threats detected.\"\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    pdf_urls = query_semantic_scholar(refined_query) + query_google_scholar(refined_query)\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "    texts, embeddings = extract_texts_and_embed(DOCUMENTS_FOLDER)\n",
    "    top_docs = rerank_documents(refined_query, texts, embeddings)\n",
    "    explanations = explain_top_documents(top_docs)\n",
    "    for (doc, score), explanation in zip(top_docs, explanations):\n",
    "        print(f\"Document snippet: {doc[:300]}\\nScore: {score}\\nExplanation: {explanation}\\n\")\n",
    "    intelligent_gnn_visualization(texts, embeddings)\n",
    "    decision = orchestrator(refined_query, texts)\n",
    "    print(f\"Orchestrator Decision: {decision}\")\n",
    "\n",
    "# Example Execution\n",
    "initial_query = \"Tactics, Techniques, and Procedures in ML Security\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Enhanced Graph Visualization Agent with Intelligence\n",
    " Below, we add intelligence to the Graph Visualization by incorporating insights from the Generative Reasoning Agent:\n",
    "\n",
    " What makes it Intelligent? \n",
    " - It integrates the Generative Reasoner Agent to annotate each node (document) with meaningful insights extracted through NLP.\n",
    " - Uncovers subtle and nuanced patterns in the data using semantic similarity enhanced by contextual reasoning from the LLM.\n",
    "\n",
    "'''\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def intelligent_graph_visualization(texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    sim_matrix = util.cos_sim(embeddings, embeddings).numpy()\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Nodes represent documents\n",
    "    for idx, _ in enumerate(texts):\n",
    "        G.add_node(f'Doc_{idx+1}')\n",
    "\n",
    "    # Add edges based on high similarity (threshold of 0.5)\n",
    "    threshold = 0.5\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            if sim_matrix[i, j] >= threshold:\n",
    "                G.add_edge(f'Doc_{i+1}', f'Doc_{j+1}', weight=sim_matrix[i, j])\n",
    "\n",
    "    # Use Generative Reasoner to annotate nodes intelligently\n",
    "    insights = []\n",
    "    for doc in texts:\n",
    "        insight = generative_reasoner(\n",
    "            \"Identify the TTPs, vulnerabilities, and lifecycle stages mentioned.\", [doc]\n",
    "        )\n",
    "        insights.append(insight)\n",
    "\n",
    "    # Visualize the graph with annotations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G, k=0.3)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='gray', node_size=1500)\n",
    "    \n",
    "    # Annotate nodes with intelligent insights\n",
    "    node_labels = {f'Doc_{idx+1}': insight[:50] + \"...\" for idx, insight in enumerate(insights)}\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8)\n",
    "\n",
    "    edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={k: f\"{v:.2f}\" for k, v in edge_labels.items()})\n",
    "\n",
    "    plt.title(\"Intelligent Visualization of Relationships (TTPs, Vulnerabilities, ML Lifecycles)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A complete Multi-Agent RAG Workflow\n",
    "'''\n",
    "def main_agent(initial_query):\n",
    "    # Step 1: Refine Query\n",
    "    refined_query = refine_query(initial_query)\n",
    "    \n",
    "    # Step 2 & 3: Retrieve PDFs (Semantic Scholar + Google Scholar)\n",
    "    pdf_urls = query_semantic_scholar(refined_query) + query_google_scholar(refined_query)\n",
    "    \n",
    "    # Step 4: Download PDFs\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "    \n",
    "    # Step 5: Extract texts and embed PDFs\n",
    "    texts, embeddings = extract_texts_and_embed(DOCUMENTS_FOLDER)\n",
    "    \n",
    "    # Step 6: Rank and explain top documents\n",
    "    ranked_docs_with_explanations = rank_and_explain(refined_query, texts, embeddings)\n",
    "    \n",
    "    for doc, explanation in ranked_docs_with_explanations:\n",
    "        print(f\"Document snippet: {doc[:300]}\")\n",
    "        print(f\"Explanation: {explanation}\\n\")\n",
    "\n",
    "    # Step 7: Intelligent graph visualization\n",
    "    intelligent_graph_visualization(texts)\n",
    "\n",
    "    # Step 8: Proactive orchestration (threat detection & response)\n",
    "    decision = orchestrator(refined_query, embeddings, texts)\n",
    "    print(f\"Orchestrator Decision: {decision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Autonomous Multi-Agent RAG System\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scholarly import scholarly\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: LLM Query Refinement Agent\n",
    "def refine_query_llm(initial_query):\n",
    "    prompt = f\"Refine this query for academic research retrieval: '{initial_query}'\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Step 2: Semantic Scholar Agent\n",
    "def query_semantic_scholar(query, limit=3):\n",
    "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": query, \"limit\": limit, \"fields\": \"openAccessPdf\"}\n",
    "    response = requests.get(api_url, params=params).json()\n",
    "    return [p['openAccessPdf']['url'] for p in response.get(\"data\", []) if p.get('openAccessPdf')]\n",
    "\n",
    "# Step 3: Query Google Scholar\n",
    "def query_google_scholar(refined_query, num_results=3):\n",
    "    search_results = scholarly.search_pubs(refined_query)\n",
    "    pdf_urls = []\n",
    "    for _ in range(num_results):\n",
    "        pub = next(search_results, None)\n",
    "        if pub and 'eprint_url' in pub:\n",
    "            pdf_urls.append(pub['eprint_url'])\n",
    "    return pdf_urls\n",
    "\n",
    "# Download PDFs\n",
    "def download_pdfs(pdf_urls, folder):\n",
    "    for idx, pdf_url in enumerate(pdf_urls):\n",
    "        try:\n",
    "            pdf_response = requests.get(pdf_url)\n",
    "            pdf_response.raise_for_status()\n",
    "            filename = os.path.join(folder, f'document_{idx}.pdf')\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(pdf_response.content)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "# Extract texts and embed PDFs\n",
    "def extract_texts_and_embed(folder):\n",
    "    texts = []\n",
    "    for pdf_file in os.listdir(folder):\n",
    "        with pdfplumber.open(os.path.join(folder, pdf_file)) as pdf:\n",
    "            text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "            texts.append(text)\n",
    "    embeddings = SentenceTransformer('all-MiniLM-L6-v2').encode(texts)\n",
    "    return texts, embeddings\n",
    "\n",
    "# Generative Reasoning Agent\n",
    "def generative_reasoner(query, context_docs):\n",
    "    prompt = f\"Context: {' '.join(context_docs)}\\n\\nUser Query: {query}\\n\\nDetailed Answer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Detection Agent\n",
    "def detect_attack(query):\n",
    "    keywords = [\"evasion\", \"extraction\", \"backdoor\", \"poison\", \"gradient masking\"]\n",
    "    return any(keyword in query.lower() for keyword in keywords)\n",
    "\n",
    "# Orchestrator Agent\n",
    "def orchestrator(query, texts, embeddings):\n",
    "    if detect_attack(query):\n",
    "        analysis = generative_reasoner(query, texts)\n",
    "        return f\"Threat detected! Actions recommended: {analysis}\"\n",
    "    return \"No threats detected.\"\n",
    "\n",
    "# Intelligent Graph Visualization\n",
    "def intelligent_graph_visualization(texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i, doc in enumerate(texts):\n",
    "        G.add_node(f\"Doc_{i+1}\")\n",
    "        for j in range(i+1, len(texts)):\n",
    "            similarity = sim_matrix[i, j]\n",
    "            if similarity > 0.5:\n",
    "                G.add_edge(f\"Doc_{i+1}\", f\"Doc_{j+1}\", weight=similarity)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=700, node_color=\"skyblue\")\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f\"{d['weight']:.2f}\" for u, v, d in G.edges(data=True)})\n",
    "    plt.title(\"Intelligent Relationship Graph\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    pdf_urls = query_semantic_scholar(refined_query) + query_google_scholar(refined_query)\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "    texts, embeddings = extract_texts_and_embed(DOCUMENTS_FOLDER)\n",
    "    orchestrator_decision = orchestrator(refined_query, texts, embeddings)\n",
    "    print(f\"Orchestrator Decision: {orchestrator_decision}\")\n",
    "    visualize_relationships(texts)\n",
    "\n",
    "# Example Execution\n",
    "initial_query = \"Tactics, Techniques, and Procedures in ML Security\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# LLM-based query refinement\n",
    "def refine_query_with_llm(initial_query):\n",
    "    prompt = (\n",
    "        \"You're an AI specialized in cybersecurity and machine learning.\\n\"\n",
    "        \"Refine and expand the following query into a structured and detailed academic-style query \"\n",
    "        \"that will retrieve research papers specifically about adversarial threats, vulnerabilities, \"\n",
    "        \"and attacks throughout the ML lifecycle:\\n\\n\"\n",
    "        f\"Initial Query: {initial_query}\\n\\n\"\n",
    "        \"Refined Query:\"\n",
    "    )\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=150\n",
    "    )\n",
    "\n",
    "    refined_query = response.choices[0].text.strip()\n",
    "    return refined_query\n",
    "\n",
    "# Example use\n",
    "initial_query = \"Tactics, Techniques, and Procedures in ML security\"\n",
    "refined_query = refine_query(initial_query)\n",
    "print(f\"LLM Refined Query: {refined_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Scholar search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Multi-Agent RAG System with Semantic Scholar Integration\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "SEMANTIC_SCHOLAR_API = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Step 1: Query Refinement via LLM\n",
    "\n",
    "def refine_query(initial_query):\n",
    "    prompt = (\n",
    "        f\"Refine the following query into a structured search query suitable for academic databases: '{initial_query}'\"\n",
    "    )\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Semantic Scholar Query Agent\n",
    "def query_semantic_scholar(refined_query, limit=5):\n",
    "    params = {\n",
    "        \"query\": refined_query,\n",
    "        \"limit\": limit,\n",
    "        \"fields\": \"url,openAccessPdf\"\n",
    "    }\n",
    "    response = requests.get(f\"{SEMANTIC_SCHOLAR_API}?{urlencode(params)}\")\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    urls = [paper[\"openAccessPdf\"][\"url\"] for paper in data.get(\"data\", []) if paper.get(\"openAccessPdf\")]\n",
    "    return urls\n",
    "\n",
    "# PDF Download Agent\n",
    "def download_pdfs(urls, folder):\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    for url in urls:\n",
    "        try:\n",
    "            pdf_response = requests.get(url)\n",
    "            pdf_response.raise_for_status()\n",
    "            filename = os.path.join(folder, url.split('/')[-1])\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "            print(f\"Downloaded and saved: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "# Main workflow\n",
    "def main_agent(initial_query, folder=Path(DOCUMENTS_FOLDER), limit=5):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    print(f\"Refined query: {refined_query}\")\n",
    "\n",
    "    print(\"Querying Semantic Scholar...\")\n",
    "    pdf_urls = query_semantic_scholar(refined_query, limit=limit)\n",
    "\n",
    "    print(\"Downloading PDFs...\")\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    download_pdfs(pdf_urls, folder)\n",
    "\n",
    "# Example execution\n",
    "initial_query = \"TTPs for adversarial ML throughout the lifecycle\"\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "limit = 5\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Scholar search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Multi-Agent RAG System with Google Scholar Integration\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from scholarly import scholarly\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# LLM-based query refinement\n",
    "def refine_query(initial_query):\n",
    "    prompt = (\n",
    "        f\"Refine the following query into a structured search query suitable for academic databases: '{initial_query}'\"\n",
    "    )\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Query Google Scholar using the scholarly library\n",
    "def query_google_scholar(refined_query, num_results=5):\n",
    "    from scholarly import scholarly\n",
    "\n",
    "    search_results = scholarly.search_pubs(refined_query)\n",
    "    pdf_urls = []\n",
    "\n",
    "    for _ in range(num_results):\n",
    "        try:\n",
    "            pub = next(search_results)\n",
    "            if 'eprint_url' in pub:\n",
    "                pdf_url = pub['eprint_url']\n",
    "                if pdf_url.endswith('.pdf'):\n",
    "                    pdf_url = pdf_url\n",
    "                else:\n",
    "                    pdf_url = pdf_url + '.pdf'\n",
    "                pdf_urls.append(pdf_url)\n",
    "            elif 'pub_url' in pub['bib']:\n",
    "                pdf_urls.append(pub['bib']['pub_url'])\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return pdf_urls\n",
    "\n",
    "# Download PDFs\n",
    "def download_pdfs(pdf_urls, folder):\n",
    "    for i, pdf_url in enumerate(pdf_urls, 1):\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            response.raise_for_status()\n",
    "            filename = os.path.join(folder, f'document_{i+1}.pdf')\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded and saved: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query, folder=DOCUMENTS_FOLDER):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    print(f\"Refined query: {refined_query}\")\n",
    "\n",
    "    print(\"Querying Google Scholar...\")\n",
    "    pdf_urls = query_google_scholar(refined_query)\n",
    "\n",
    "    print(\"Downloading PDFs...\")\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "\n",
    "# Example usage\n",
    "initial_query = \"TTPs for adversarial ML\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Multi-Agent Advanced RAG System with Semantic & Google Scholar Integration\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from scholarly import scholarly\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Query Refinement via LLM\n",
    "\n",
    "def refine_query(initial_query):\n",
    "    prompt = (\n",
    "        f\"Refine the following query into a structured search query suitable for academic databases: '{initial_query}'\"\n",
    "    )\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Step 2: Query Semantic Scholar\n",
    "def query_semantic_scholar(refined_query, limit=5):\n",
    "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": refined_query, \"limit\": limit, \"fields\": \"url,openAccessPdf\"}\n",
    "    response = requests.get(api_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    urls = [paper[\"openAccessPdf\"][\"url\"] for paper in data.get(\"data\", []) if paper.get(\"openAccessPdf\")]\n",
    "    return urls\n",
    "\n",
    "# Step 3: Query Google Scholar\n",
    "def query_google_scholar(refined_query, num_results=5):\n",
    "    search_results = scholarly.search_pubs(refined_query)\n",
    "    pdf_urls = []\n",
    "    for _ in range(num_results):\n",
    "        try:\n",
    "            pub = next(search_results)\n",
    "            if 'eprint_url' in pub:\n",
    "                pdf_urls.append(pub['eprint_url'])\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return pdf_urls\n",
    "\n",
    "# Step 4: Download PDFs\n",
    "def download_pdfs(pdf_urls, folder):\n",
    "    for i, pdf_url in enumerate(pdf_urls):\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            response.raise_for_status()\n",
    "            filename = os.path.join(folder, f'document_{i+1}.pdf')\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "# Step 5: Extract and Embed PDFs\n",
    "def extract_texts_and_embed(folder):\n",
    "    texts = []\n",
    "    for pdf_file in os.listdir(folder):\n",
    "        with pdfplumber.open(os.path.join(folder, pdf_file)) as pdf:\n",
    "            text = ''.join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "            texts.append(text)\n",
    "    embeddings = model.encode(texts)\n",
    "    return texts, embeddings\n",
    "\n",
    "# Step 6: Document Ranking and Explainability\n",
    "def rank_and_explain(query, texts, embeddings):\n",
    "    query_embedding = model.encode([query])\n",
    "    scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "    ranked_indices = scores.argsort(descending=True)\n",
    "\n",
    "    explainer = LimeTextExplainer()\n",
    "    explanations = []\n",
    "    for idx in ranked_indices[:3]:\n",
    "        exp = explainer.explain_instance(texts[idx], lambda x: model.encode(x))\n",
    "        explanations.append(exp.as_list())\n",
    "\n",
    "    return [texts[i] for i in ranked_indices[:3]], explanations\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    pdf_urls = query_semantic_scholar(refined_query) + query_google_scholar(refined_query)\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "    texts, embeddings = extract_texts_and_embed(DOCUMENTS_FOLDER)\n",
    "    top_texts, explanations = rank_and_explain(refined_query, texts, embeddings)\n",
    "    print(\"Top documents and explanations:\")\n",
    "    for text, explanation in zip(top_texts, explanations):\n",
    "        print(text[:500])\n",
    "        print(\"Explanation:\", explanation)\n",
    "\n",
    "# Example usage\n",
    "initial_query = \"TTPs for adversarial ML security\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Multi-Agent RAG System with Graph Visualization\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scholarly import scholarly\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "Path(DOCUMENTS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Query Refinement via LLM\n",
    "def refine_query(initial_query):\n",
    "    refined_query = \"((\\\"Tactics, Techniques, and Procedures in Machine Learning Security\\\") AND (\\\"adversarial attacks\\\" OR \\\"data poisoning\\\" OR \\\"evasion attack\\\" OR \\\"model tampering\\\" OR \\\"model inversion\\\" OR \\\"backdoor attack\\\" OR \\\"adversarial example\\\"))\"\n",
    "    return refined_query\n",
    "\n",
    "# Step 2: Query Semantic Scholar\n",
    "# (Implementation not provided here, assumed from previous examples)\n",
    "\n",
    "# Step 3: Query Google Scholar\n",
    "def query_google_scholar(refined_query, num_results=5):\n",
    "    search_results = scholarly.search_pubs(refined_query)\n",
    "    pdf_urls = []\n",
    "    for _ in range(num_results):\n",
    "        try:\n",
    "            pub = next(search_results)\n",
    "            if 'eprint_url' in pub:\n",
    "                pdf_urls.append(pub['eprint_url'])\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return pdf_urls\n",
    "\n",
    "# PDF Download Agent\n",
    "def download_pdfs(pdf_urls, folder):\n",
    "    for i, pdf_url in enumerate(pdf_urls):\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            response.raise_for_status()\n",
    "            with open(Path(folder) / f'doc_{i}.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded: doc_{i}.pdf\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {pdf_url}: {e}\")\n",
    "\n",
    "# Extract texts and embed PDFs\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def extract_texts_and_embed(folder):\n",
    "    texts = []\n",
    "    for pdf_file in os.listdir(folder):\n",
    "        with pdfplumber.open(Path(folder) / pdf_file) as pdf:\n",
    "            text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "            texts.append(text)\n",
    "    embeddings = model.encode(texts)\n",
    "    return texts, embeddings\n",
    "\n",
    "# Rank and Explain Agent\n",
    "def rank_and_explain(query, texts, embeddings):\n",
    "    query_emb = model.encode([query])\n",
    "    scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "    ranked_indices = scores.argsort(descending=True)\n",
    "\n",
    "    explainer = LimeTextExplainer()\n",
    "    explanations = [explainer.explain_instance(texts[i], lambda x: model.encode(x), num_features=5).as_list() for i in ranked_indices[:3]]\n",
    "\n",
    "    return [(texts[i], explanations[idx]) for idx, i in enumerate(ranked_indices[:3])]\n",
    "\n",
    "# Graph Visualization Agent\n",
    "def visualize_relationships(texts):\n",
    "    entities = ['TTP', 'Vulnerability', 'ML Lifecycle']\n",
    "    embeddings = model.encode(texts)\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    graph = {}\n",
    "\n",
    "    for i, doc in enumerate(texts):\n",
    "        graph[f\"Doc_{i}\"] = {}\n",
    "        for j, sim_score in enumerate(sim_scores):\n",
    "            if i != j and sim_score > 0.5:\n",
    "                graph[f\"Doc_{i}\"][f\"Doc_{j}\"] = sim_score\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for doc, edges in graph.items():\n",
    "        for target, weight in edges.items():\n",
    "            G.add_edge(doc, target, weight=weight)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G, k=0.3)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='gray')\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={k: f'{v:.2f}' for k, v in labels.items()})\n",
    "    plt.title(\"Relationships Between TTPs, Vulnerabilities, and ML lifecycle stages\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    pdf_urls = query_semantic_scholar(refined_query) + query_google_scholar(refined_query)\n",
    "    download_pdfs(pdf_urls, DOCUMENTS_FOLDER)\n",
    "    texts, embeddings = extract_texts_and_embed(DOCUMENTS_FOLDER)\n",
    "    top_docs_with_explanations = rank_and_explain(refined_query, texts, embeddings)\n",
    "\n",
    "    print(\"Top Documents and Explanations:\")\n",
    "    for doc, explanation in top_docs_with_explanations:\n",
    "        print(doc[:300])\n",
    "        print(\"Explanation:\", explanation)\n",
    "\n",
    "    # Graph Visualization\n",
    "    visualize_graph(texts, embeddings)\n",
    "\n",
    "# Example Execution\n",
    "initial_query = \"TTPs in Machine Learning Security\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Agentic RAG System Example\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Mock Document store: represents simplified knowledge-base.\n",
    "DOCUMENTS = [\n",
    "    \"Evasion attacks manipulate test samples at inference time to evade detection.\",\n",
    "    \"Model extraction attacks use queries to steal ML models.\",\n",
    "    \"Backdoor attacks poison ML training data with hidden triggers.\",\n",
    "    \"Gradient masking defends ML models by obfuscating gradients.\",\n",
    "    \"Parseval networks improve robustness by controlling the Lipschitz constant.\",\n",
    "    \"Transferability attacks craft adversarial examples on surrogate models to attack unknown models.\",\n",
    "    \"Membership inference attacks infer if data points belong to training sets.\",\n",
    "]\n",
    "\n",
    "# Initialize TTP knowledge base vector embeddings (simplified)\n",
    "vector_db = TfidfVectorizer().fit_transform(DOCUMENTS)\n",
    "\n",
    "# Retriever Agent: fetches relevant information\n",
    "def retrieve_docs(query, vector_db, documents, top_k=3):\n",
    "    query_vec = TfidfVectorizer().fit(documents).transform([query])\n",
    "    scores = cosine_similarity(query, vector_db).flatten()\n",
    "    top_indices = scores.argsort()[::-1][:top_k]\n",
    "    return [documents[i] for i in top_k_indices]\n",
    "\n",
    "# Generative Reasoning Agent: makes sense of retrieved docs\n",
    "def generative_reasoner(query, context_docs):\n",
    "    prompt = f\"Context: {' '.join(context_docs)}\\n\\nUser Query: {query}\\n\\nAnswer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Detection Agent: determines if query indicates an adversarial attack\n",
    "def detect_attack(query):\n",
    "    keywords = [\"evasion\", \"extraction\", \"backdoor\", \"poison\", \"gradient masking\"]\n",
    "    return any(keyword in query.lower() for keyword in keywords)\n",
    "\n",
    "# Orchestrator Agent: decides on response based on detection results\n",
    "def orchestrator(query, vector_db, documents):\n",
    "    if detect_threat := detect_attack(query):\n",
    "        context_docs = retrieve_docs(query, vector_db, DOCUMENTS)\n",
    "        analysis = generative_reasoner(query, context_docs=context_docs)\n",
    "        decision = f\"Threat detected! Taking security action. Details: {response}\"\n",
    "    else:\n",
    "        decision = \"No threats detected. No action needed.\"\n",
    "    return decision\n",
    "\n",
    "# Learner Agent: placeholder to demonstrate continual learning\n",
    "def update_knowledge_base(new_doc, documents):\n",
    "    documents.append(new_doc)\n",
    "    return TfidfVectorizer().fit_transform(documents)\n",
    "\n",
    "# Main workflow\n",
    "def main_agent(query):\n",
    "    print(f\"Received query: {query}\")\n",
    "    docs = retrieve_docs(query, vector_db, DOCUMENTS)\n",
    "    response = orchestrator(query, vector_db, DOCUMENTS)\n",
    "    print(response)\n",
    "\n",
    "# Example query (model extraction threat scenario)\n",
    "query = \"We observed numerous queries potentially indicative of model extraction.\"\n",
    "main_response = orchestrator(query, vector_db, DOCUMENTS)\n",
    "print(main_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Agentic RAG System for ML Security\n",
    "\n",
    "import openai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Knowledge Base: Security knowledge docs\n",
    "documents = [\n",
    "    \"Evasion attacks manipulate inputs to evade detection by ML models.\",\n",
    "    \"Model extraction attacks use queries to steal ML model information.\",\n",
    "    \"Backdoor attacks poison ML training data with hidden triggers.\",\n",
    "    \"Gradient masking defends ML models by obfuscating gradients.\",\n",
    "    \"Parseval networks improve robustness by controlling the Lipschitz constant.\",\n",
    "    \"Adversarial examples exploit vulnerabilities in ML models.\",\n",
    "    \"Data poisoning introduces malicious data to corrupt ML training processes.\",\n",
    "    \"Evasion attacks involve subtle perturbations to avoid detection at inference.\" \n",
    "}\n",
    "\n",
    "# Vectorize documents for semantic retrieval\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector_db = vectorizer.fit_transform(DOCUMENTS)\n",
    "\n",
    "# Retriever Agent\n",
    "def retrieve_docs(query, vectorizer, vector_db, documents, top_k=3):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, vector_db).flatten()\n",
    "    top_indices = scores.argsort()[::-1][:top_k]\n",
    "    return [documents[i] for i in top_k]\n",
    "\n",
    "# Generative Reasoning Agent (LLM)\n",
    "def generative_reasoning(query, context_docs):\n",
    "    prompt = f\"Context: {' '.join(context_docs)}\\n\\nUser Query: {query}\\n\\nAnswer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Detection Agent\n",
    "def detect_attack(query):\n",
    "    keywords = [\"data poisoning\", \"evasion\", \"model extraction\", \"tampering\", \"gradient masking\"]\n",
    "    return any(keyword in query.lower() for keyword in keywords)\n",
    "\n",
    "# Orchestrator Agent\n",
    "def orchestrator(query, vectorizer, vector_db, documents):\n",
    "    context_docs = retrieve_docs(query, vectorizer, vector_db, documents)\n",
    "    if detect_attack(query):\n",
    "        action = \"Adversarial threat detected. Initiating defensive actions.\"\n",
    "    else:\n",
    "        action = \"No immediate threat detected. Proceeding with standard operations.\"\n",
    "    generated_response = Generative_Reasoning_Agent(query, context_docs)\n",
    "    return f\"{action}\\n\\nDetailed Analysis:\\n{generated_response}\"\n",
    "\n",
    "# Query Refinement Agent (to adapt initial user query into detailed RAG queries)\n",
    "def refine_query(initial_query):\n",
    "    refined_query = (\n",
    "        \"(adversarial OR threat OR vulnerability) AND \"\n",
    "        \"(machine learning security OR ML security OR AI security OR deep learning security) AND \"\n",
    "        \"(data poisoning OR evasion attack OR model tampering)\"\n",
    "    )\n",
    "    return refined_query\n",
    "\n",
    "# Main Workflow Agent\n",
    "def main_agent(initial_query):\n",
    "    print(f\"Received initial query: {initial_query}\")\n",
    "    detailed_query = refine_query(initial_query)\n",
    "    response = orchestrator(detailed_query, vectorizer, vector_db, DOCUMENTS)\n",
    "    print(response)\n",
    "\n",
    "# Example Execution\n",
    "initial_query = \"TTPs for machine learning security threats\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "\n",
    "# Comprehensive Multi-Agent RAG System\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Document Repository\n",
    "DOCUMENTS_FOLDER = './ML_Threat_Documents/'\n",
    "\n",
    "# Initial Query Refinement via Retrieval Augmentation\n",
    "def refine_query(initial_query):\n",
    "    refined_query = (\n",
    "        \"(adversarial attack OR threat OR vulnerability OR advers*) AND \"\n",
    "        \"(machine learning security OR ML security OR AI security OR deep learning security) AND \"\n",
    "        \"(data poisoning OR evasion attack OR model tampering OR data extraction)\"\n",
    "    )\n",
    "    return refined_query\n",
    "\n",
    "# Document Retriever: Fetch and Store PDFs\n",
    "def download_and_store_documents(query):\n",
    "    # This function would realistically interface with a database/API (e.g., Google Scholar API)\n",
    "    # Placeholder: simulating document retrieval and storage\n",
    "    print(f\"Documents retrieved for query: {query}\")\n",
    "    print(f\"Storing documents in: {DOCUMENTS_FOLDER}\")\n",
    "\n",
    "# Document Processing & Vector Database Update\n",
    "def process_documents(folder):\n",
    "    documents = [\n",
    "        \"Evasion attacks manipulate test-time data to fool ML classifiers.\",\n",
    "        \"Data poisoning affects training datasets to manipulate model training.\",\n",
    "        \"Backdoor attacks introduce secret triggers into ML models.\",\n",
    "    ]\n",
    "    vector_db = TfidfVectorizer().fit_transform(documents)\n",
    "    return vector_db, documents\n",
    "\n",
    "# Retriever Agent\n",
    "def retrieve_docs(query, vector_db, documents, top_k=3):\n",
    "    query_vec = TfidfVectorizer().fit(documents).transform([query])\n",
    "    scores = cosine_similarity(query_vec, vector_db).flatten()\n",
    "    top_indices = scores.argsort()[-top_k:][::-1]\n",
    "    return [documents[i] for i in top_indices]\n",
    "\n",
    "# Generative Reasoning Agent (GRA)\n",
    "def generate_response(query, context_docs):\n",
    "    prompt = f\"Context: {' '.join(context_docs)}\\n\\nUser Query: {query}\\n\\nAnswer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "\n",
    "# Sentry/Detection Agent\n",
    "def detect_attack(query):\n",
    "    keywords = [\"data poisoning\", \"evasion\", \"backdoor\", \"model extraction\", \"tampering\", \"attack\"]\n",
    "    return any(keyword in query.lower() for keyword in keywords)\n",
    "\n",
    "# Orchestrator Agent\n",
    "def orchestrator(query, vector_db, documents):\n",
    "    if detect_attack(query):\n",
    "        context_docs = retrieve_docs(query, vector_db, documents)\n",
    "        answer = retrieve_and_generate(query, context_docs)\n",
    "        print(f\"Threat detected: {answer}\")\n",
    "    else:\n",
    "        print(\"No threats detected.\")\n",
    "\n",
    "# Learner Agent\n",
    "def update_knowledge_base(new_doc, documents):\n",
    "    documents.append(new_doc)\n",
    "    return TfidfVectorizer().fit_transform(documents)\n",
    "\n",
    "# Main Workflow\n",
    "def main_agent(initial_query):\n",
    "    refined_query = refine_query(initial_query)\n",
    "    download_docs(refined_query)\n",
    "    vector_db, documents = process_docs(DOCUMENTS_FOLDER)\n",
    "    response = orchestrator(refined_query, vector_db, documents)\n",
    "    print(response)\n",
    "\n",
    "# Example execution\n",
    "initial_query = \"TTPs in ML security\"\n",
    "main_agent(initial_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to read all PDFs from a folder\n",
    "def read_pdfs_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file=file_path, mode='rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page in range(len(reader.pages)):\n",
    "                    pdf_page = reader.pages[page]\n",
    "                    documents.append(pdf_page.extract_text())\n",
    "    return documents\n",
    "\n",
    "# Generate embeddings from documents\n",
    "def generate_embeddings(documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(documents)\n",
    "    return embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"path/to/pdf/folder\"  # Update with your PDFs folder path\n",
    "    documents = read_pdfs_from_folder(folder_path)\n",
    "    embeddings = generate_embeddings(documents)\n",
    "    print(\"Embeddings generated for all PDF documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Document Embedding\n",
    "def build_embeddings(docs):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(docs)\n",
    "    return embeddings, docs\n",
    "\n",
    "# Step 2: Retrieval Agent\n",
    "def retrieve_context(query, embeddings, docs, top_k=3):\n",
    "    query_vec = model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_vec], embeddings)[0]\n",
    "    indices = similarities.argsort()[-top_k:][::-1]\n",
    "    return [docs[i] for i in indices]\n",
    "\n",
    "# Step 3: Generative Reasoning Agent\n",
    "def generate_response(query, context):\n",
    "    context_text = \"\\n\\n\".join(context)\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant helping identify and explain ML threats.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: {query}\\n\\nContext: {context}\\n\\nResponse:\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Step 3: Detection Agent\n",
    "def detect_threat(input_data, detection_model, threshold=0.7):\n",
    "    risk_score = detection_model.predict_proba([input_data])[0].max()\n",
    "    return risk_score < threshold\n",
    "\n",
    "# Step 3: Decision Agent\n",
    "def orchestrate_response(detection_flag, insights):\n",
    "    if detection_flag:\n",
    "        return f\"ðŸš¨ Threat Detected! Recommended Response: {insights}\"\n",
    "    else:\n",
    "        return \"âœ… Input appears safe. No action required.\"\n",
    "\n",
    "# Main Execution Example\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\"content from your PDFs goes here\"]\n",
    "    embeddings, docs = build_embeddings(documents)\n",
    "    \n",
    "    query = \"Describe vulnerabilities related to model extraction.\"\n",
    "    \n",
    "    context_docs = retrieve_context(query, embeddings, documents, top_k=3)\n",
    "    \n",
    "    insights = generate_insight(query, context_docs=context)\n",
    "    \n",
    "    sample_input = [0.85]  # Example detection confidence\n",
    "    detection_flag = detect_attack(sample_input, threshold=0.90)\n",
    "    \n",
    "    final_response = orchestrate_response(detection_flag, insights)\n",
    "    print(final_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  **Creating an End-to-End Integration:**\n",
    "- Embed your provided PDF contents using sentence transformers and store them in vector databases like FAISS.\n",
    "- Continuously integrate new research or data.\n",
    "- Apply real-time monitoring and alerting through dashboards (Grafana, Prometheus).\n",
    "\n",
    "This structured system allows effective utilizeation of retrieval-augmented generation in an agentic manner, proactively responding to ML security threats in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"lifecycle_stages\": {\n",
    "    \"Deployment\": [\"Evasion Attacks\", \"Model Extraction\", \"Side-Channel Attacks\", \"Sponge Attacks\", \"Graph-Based Threat Exploits\", \"HOUYI Prompt Injection Attacks\", \"QA-Prompt\", \"MASTERKEY Automated Jailbreaking\", \"Semantic Firewall Bypass (Self-Deception Attacks)\", \"Dynamic Role Hijacking Attacks\", \"LLM-System Exploits (LLMs)\", \"System Development\", \"LLM Deployment Exploitation\"],\n",
    "    \"Test time\": [\"Evasion Attacks\", \"Side-Channel Attacks\", \"Sponge Attacks\", \"Universal Adversarial Texts (UATs)\", \"Gradient-Based NLP Adversarial Attacks\", \"HOUYI Prompt Injection Attacks\", \"Visual Adversarial Examples in Multimodal Models\", \"MASTERKEY Automated Jailbreaking\", \"Systematic Jailbreak Prompts\", \"Semantic Firewall Bypass (Self-Deception Attacks)\", \"RAIN Gradient-Based Obstinate Adversarial Attacks\", \"Dynamic Role Hijacking Attacks\"],\n",
    "    \"Fine-tuning\": [\"Prompt Injection Attacks\", \"Universal Prompt Vulnerabilities\", \"Adversarial Alignment Challenges\", \"Parameter-Efficient NLP Vulnerabilities\", \"Visual Adversarial Examples in Multimodal Models\", \"Instruction-Tuning Dataset Errors (DONKII)\", \"Adversarial Alignment Challenges\", \"Universal Prompt Vulnerabilities\"],\n",
    "    \"Training\": [\"Backdoor Attacks\", \"Poisoning Attacks\", \"Federated Learning Poisoning\", \"Vertical Federated Learning Vulnerabilities\", \"Insertion-Based Backdoor Attacks\", \"Imperceptible Backdoor Attacks\", \"Instruction-Tuning Dataset Errors (DONKII)\", \"Exploiting Machine Unlearning for Backdoor Attacks (BAU)\", \"DP-Forward Robust Training\", \"Adversarial Alignment Challenges\"],\n",
    "    \"Test time\": [\"Evasion Attacks\", \"Side-Channel Attacks\", \"Adversarial Examples\", \"Sponge Attacks\", \"Universal Adversarial Texts (UATs)\", \"Gradient-Based NLP Adversarial Attacks\", \"Prompt Injection Attacks\", \"RAIN Gradient-Based Obstinate Adversarial Attacks\"],\n",
    "    \"Federated Aggregation\": [\"Federated Learning Poisoning\", \"Vertical Federated Learning Vulnerabilities\", \"FedSecurity\"],\n",
    "    \"Data Preparation\": [\"Poisoning Attacks\", \"Insertion-Based Backdoor Attacks\", \"Instruction-Tuning Dataset Errors (DONKII)\", \"Preprocessing\"],\n",
    "    \"Inference\": [\"DP-Forward Robust Training\", \"Gradient-Based Obstinate Adversarial Attacks\"],\n",
    "    \"Preprocessing\": [\"Insertion-Based Backdoor Attacks\", \"Instruction-Tuning Dataset Errors (DONKII)\"],\n",
    "    \"Post-training\": [\"Exploiting Machine Unlearning for Backdoor Attacks (BAU)\"],\n",
    "    \"Pretraining\": [\"Universal Prompt Vulnerabilities\", \"Insertion-Based Backdoor Attacks\", \"Imperceptible Backdoor Attacks\", \"Universal Prompt Vulnerabilities\"],\n",
    "    \"Federated Aggregation\": [\"Federated Learning Poisoning\", \"Federated Learning Poisoning with Federated LLMs (FedSecurity)\", \"Vertical Federated Learning Vulnerabilities\"],\n",
    "    \"System Modeling\": [\"Graph-Based Threat Exploits\"],\n",
    "    \"System Development\": [\"LLM Prompt Injection Attacks\", \"LLM System Exploits (LLMsmith)\", \"Trust Exploitation in LLM\"],\n",
    "    \"Data Collection\": [\"Instruction-Tuning Dataset Errors (DONKII)\"]\n",
    "},\n",
    "\"TTPs\": {\n",
    "    \"Evasion Attacks\": [\"Deployment\", \"Test time\"],\n",
    "    \"Model Extraction\": [\"Deployment\", \"Inference\"],\n",
    "    \"Backdoor Attacks\": [\"Training\", \"Data Preparation\"],\n",
    "    \"Membership Inference\": [\"Deployment\", \"Inference\"],\n",
    "    \"Transferability Attacks\": [\"Test time\"],\n",
    "    \"Poisoning Attacks\": [\"Training\", \"Data Preparation\"],\n",
    "    \"Side-Channel Attacks\": [\"Deployment\", \"Test time\"],\n",
    "    \"Universal Adversarial Texts\": [\"Test time\"],\n",
    "    \"Federated Learning Poisoning\": [\"Training\", \"Federated Aggregation\"]\n",
    "},\n",
    "\"Vulnerabilities\": [\n",
    "    \"Weak Adversarial Defenses\", \"Insufficient Monitoring\", \"Data Poisoning\", \"Unencrypted Model Parameters\",\n",
    "    \"Privacy Violations\", \"Exposed Training Data\", \"Biased Data Sources\", \"Semantic Exploitation\",\n",
    "    \"Resource Exhaustion\", \"Trigger Sensitivity\", \"Input Validation Gaps\", \"Model Overfitting\",\n",
    "    \"Federated Model Poisoning\", \"Backdoor Exploits\", \"Model Overfitting\", \"Exposed Training Data\"\n",
    "],\n",
    "\"Critical_Vulnerabilities\": [\n",
    "    \"Data Poisoning\", \"Exposed Training Data\", \"Privacy Violations\", \"Model Overfitting\",\n",
    "    \"Federated Model Poisoning\", \"Backdoor Exploits\"\n",
    "]\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
